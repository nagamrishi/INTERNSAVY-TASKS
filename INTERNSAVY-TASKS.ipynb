{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"4.Cricket Player Performance Prediction\nusing machine learning","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime\nimport matplotlib.pyplot as plt   # Import matplotlib\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\ndf = pd.read_csv('../input/ODI_Match_Results.csv') \n\n\ny = df['Result']\n\nX = df.loc[:,['Margin','Toss','Bat','Opposition','Ground','Country']]\n\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X = LabelEncoder()\n#X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\nonehotencoder = OneHotEncoder()\n#X = X.replace('<', '', regex=True)\nX = onehotencoder.fit_transform(X).toarray()\n# Encoding the Dependent Variable\nlabelencoder_y = LabelEncoder()\ny = labelencoder_y.fit_transform(y)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n\n# Applying LDA\n#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n#lda = LDA(n_components = 2)\n#X_train = lda.fit_transform(X_train, y_train)\n#X_test = lda.transform(X_test)\n\n# Applying PCA\n#from sklearn.decomposition import PCA\n#pca = PCA(n_components = 2)\n#X_train = pca.fit_transform(X_train)\n#X_test = pca.transform(X_test)\n#explained_variance = pca.explained_variance_ratio_\n#explained_variance\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\ny_pred\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ny_test\nprint(cm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"3.Customer Segmentation Analysis\nwith Python","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n​\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n​\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n​\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n​\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nadd Codeadd Markdown\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom sklearn.cluster import KMeans\nadd Codeadd Markdown\n#loadong dataset\ndf = pd.read_csv('/kaggle/input/customer-segmentation-tutorial-in-python/Mall_Customers.csv')\nadd Codeadd Markdown\ndf.head()\nadd Codeadd Markdown\ndf.shape\nadd Codeadd Markdown\ndf.info\nadd Codeadd Markdown\ndf.info()\nadd Codeadd Markdown\n#checking missing values\ndf.isnull().sum()\nadd Codeadd Markdown\n#brief overview of data\ndf.describe()\nadd Codeadd Markdown\n#checking the datatype of all factors\ndf.dtypes\nadd Codeadd Markdown\n#dropping customer id column as it is of no use \ndf.drop([\"CustomerID\"],axis=1,inplace=True)\nadd Codeadd Markdown\ndf.head()\nadd Codeadd Markdown\n#Analysis and visualisation of data\nplt.figure(1,figsize=(15,6))\nn = 0\nfor x in ['Age','Annual Income (k$)','Spending Score (1-100)']:\n    n +=1\n    plt.subplot(1,3,n)\n    plt.subplots_adjust(hspace = 0.5,wspace = 0.5)\n    sns.distplot(df[x],bins=20)\n    plt.title('Distplot of {}'.format(x))\nplt.show()\n#from the graph we infer\n#age-most people have age 25-30\n#annual income - most people have annual income 60-75k$\n#spending score - most people have score near 50\nadd Codeadd Markdown\n#comparison between no. of males and no. of females\nplt.figure(figsize=(15,5))\nsns.countplot(y ='Gender',data =df)\nplt.show()\n#We see there are more females than males \nadd Codeadd Markdown\nplt.figure(1,figsize = (15,7))\nn = 0\nfor cols in ['Age','Annual Income (k$)','Spending Score (1-100)']:\n    n += 1\n    plt.subplot(1,3,n)\n    sns.set(style = \"whitegrid\")\n    plt.subplots_adjust(hspace = 0.5,wspace = 0.5)\n    sns.violinplot(x = cols,y = 'Gender' , data = df)\n    plt.ylabel('Gender' if n == 1 else '')\n    plt.title('Violin plot')\nplt.show()\n#high density of age Vs gender at 25-35 \n#high density of annual income vs gender at 65-70k$\n#high density of spending score vs gender nearly at 50\nadd Codeadd Markdown\n#for higher understanding let divide age and see which range has more number of customers \nage_18_25 = df.Age[(df.Age >= 18) & (df.Age <= 25)]\nage_26_35 = df.Age[(df.Age >= 26) & (df.Age <= 35)]\nage_36_45 = df.Age[(df.Age >= 36) & (df.Age <= 45)]\nage_46_55 = df.Age[(df.Age >= 46) & (df.Age <= 55)]\nage_55above = df.Age[(df.Age >= 56)]\nadd Codeadd Markdown\nage_x = [\"18-25\",\"26-35\",\"36-45\",\"46-55\",\"55+\"]\nage_y = [len(age_18_25.values),len(age_26_35.values),len(age_36_45.values),len(age_46_55.values),len(age_55above.values)]\nadd Codeadd Markdown\nplt.figure(figsize = (15,6))\nsns.barplot(x = age_x , y = age_y , palette = \"mako\")\nplt.title(\"Number of customer and ages\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Number of customers\")\nplt.show()\n#we see that most numer of customers are there in 26-35 age group\nadd Codeadd Markdown\n#lets try to see relation between annual income and spending score\nsns.relplot(x = \"Annual Income (k$)\",y = \"Spending Score (1-100)\",data = df)\n#we can see that people having annual income 40-60k$ has spending score 40-60\nadd Codeadd Markdown\n#Now lets divide spending score as we divided age\nss_1_20 = df[\"Spending Score (1-100)\"][(df[\"Spending Score (1-100)\"] >= 1) & (df[\"Spending Score (1-100)\"] <= 20)]\nss_21_40 = df[\"Spending Score (1-100)\"][(df[\"Spending Score (1-100)\"] >= 21) & (df[\"Spending Score (1-100)\"] <= 40)]\nss_41_60 = df[\"Spending Score (1-100)\"][(df[\"Spending Score (1-100)\"] >= 31) & (df[\"Spending Score (1-100)\"] <= 60)]\nss_61_80 = df[\"Spending Score (1-100)\"][(df[\"Spending Score (1-100)\"] >= 61) & (df[\"Spending Score (1-100)\"] <= 80)]\nss_81_100 = df[\"Spending Score (1-100)\"][(df[\"Spending Score (1-100)\"] >= 81) & (df[\"Spending Score (1-100)\"] <= 100)]\nadd Codeadd Markdown\nss_x = [\"1-20\",\"21-40\",\"41-60\",\"61-80\",\"81-100\"]\nss_y = [len(ss_1_20.values),len(ss_21_40.values),len(ss_41_60.values),len(ss_61_80.values),len(ss_81_100.values)]\nadd Codeadd Markdown\nplt.figure(figsize = (15,6))\nsns.barplot(x = ss_x , y = ss_y , palette = \"rocket\")\nplt.title(\"Spending scores\")\nplt.xlabel(\"Scores\")\nplt.ylabel(\"Number of customers\")\nplt.show()\n#we can see that most customers have spending score 41-60\nadd Codeadd Markdown\n#Now lets divide te annual income \nai_0_30 = df[\"Annual Income (k$)\"][(df[\"Annual Income (k$)\"] >= 0) & (df[\"Annual Income (k$)\"] <= 30)]\nai_31_60 = df[\"Annual Income (k$)\"][(df[\"Annual Income (k$)\"] >= 31) & (df[\"Annual Income (k$)\"] <= 60)]\nai_61_90 = df[\"Annual Income (k$)\"][(df[\"Annual Income (k$)\"] >= 61) & (df[\"Annual Income (k$)\"] <= 90)]\nai_91_120 = df[\"Annual Income (k$)\"][(df[\"Annual Income (k$)\"] >= 91) & (df[\"Annual Income (k$)\"] <= 120)]\nai_121_150 = df[\"Annual Income (k$)\"][(df[\"Annual Income (k$)\"] >= 121) & (df[\"Annual Income (k$)\"] <= 150)]\nadd Codeadd Markdown\nai_x = [\"0-30\",\"31-60\",\"61-90\",\"91-120\",\"121-150\"]\nai_y = [len(ai_0_30.values),len(ai_31_60.values),len(ai_61_90.values),len(ai_91_120.values),len(ai_121_150.values)]\nadd Codeadd Markdown\nplt.figure(figsize = (15,6))\nsns.barplot(x = ai_x , y = ai_y , palette = \"Spectral\")\nplt.title(\"Annual incomes\")\nplt.xlabel(\"Income\")\nplt.ylabel(\"Number of customers\")\nplt.show()\n#we can see that most customers has income 61-90$\nadd Codeadd Markdown\n#Now we have analysed our data lets start with the clustering process\n#first find the no. of clusters required\n#lets consider relationship between age and spending score \nX1 = df.loc[:, [\"Age\",\"Spending Score (1-100)\"]].values\n​\nfrom sklearn.cluster import KMeans\n#find optimum no. of clusters \nwcss = []\nfor k in range(1,11):\n    kmeans = KMeans(n_clusters = k,init = \"k-means++\")\n    kmeans.fit(X1)\n    wcss.append(kmeans.inertia_)\nplt.figure(figsize = (15,6))\nplt.grid()\nplt.plot(range(1,11),wcss,linewidth=2, color=\"red\",marker = \"8\")\nplt.xlabel(\"K value\")\nplt.ylabel(\"WCSS\")\nplt.show()\n#we can see that graph goes nearly constant after kmeans = 4 \n#so lets take number of clusters  = 4\nadd Codeadd Markdown\nkmeans = KMeans(n_clusters = 4)\n​\nlabel = kmeans.fit_predict(X1)\nprint(label)\nadd Codeadd Markdown\n#lets check the centroids now \nprint(kmeans.cluster_centers_)\nadd Codeadd Markdown\n#Now lets visualise the clusters we have formed\nplt.scatter(X1[:,0],X1[:,1],c = kmeans.labels_ , cmap = 'rainbow')\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1], color = 'black')\nplt.title('Clusters of customers')\nplt.xlabel('Age')\nplt.ylabel('Spending Score(1-100)')\nplt.show()\nadd Codeadd Markdown\n#now lets consider relationship between annual income and spending score\nX2 = df.loc[:, [\"Annual Income (k$)\",\"Spending Score (1-100)\"]].values\n​\nfrom sklearn.cluster import KMeans\n#find optimum no. of clusters \nwcss = []\nfor k in range(1,11):\n    kmeans = KMeans(n_clusters = k,init = \"k-means++\")\n    kmeans.fit(X2)\n    wcss.append(kmeans.inertia_)\nplt.figure(figsize = (15,6))\nplt.grid()\nplt.plot(range(1,11),wcss,linewidth=2, color=\"red\",marker = \"8\")\nplt.xlabel(\"K value\")\nplt.ylabel(\"WCSS\")\nplt.show()\n#we can see that graph goes nearly constant after kmeans = 5\n#so lets take number of clusters  = 5\nadd Codeadd Markdown\nkmeans = KMeans(n_clusters = 5)\n​\nlabel = kmeans.fit_predict(X2)\nprint(label)\nadd Codeadd Markdown\n #lets check the centroids now \nprint(kmeans.cluster_centers_)\nadd Codeadd Markdown\n#Now lets visualise the clusters we have formed\nplt.scatter(X2[:,0],X2[:,1],c = kmeans.labels_ , cmap = 'rainbow')\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1], color = 'black')\nplt.title('Clusters of customers')\nplt.xlabel('Annual incomes (in $)')\nplt.ylabel('Spending Score(1-100)')\nplt.show()\nadd Codeadd Markdown\n#Now lets see the relationship between all the three together \nX3 = df.loc[:, [\"Age\",\"Annual Income (k$)\",\"Spending Score (1-100)\"]].values\n#find optimum no. of clusters \nwcss = []\nfor k in range(1,11):\n    kmeans = KMeans(n_clusters = k,init = \"k-means++\")\n    kmeans.fit(X3)\n    wcss.append(kmeans.inertia_)\nplt.figure(figsize = (15,6))\nplt.grid()\nplt.plot(range(1,11),wcss,linewidth=2, color=\"red\",marker = \"8\")\nplt.xlabel(\"K value\")\nplt.ylabel(\"WCSS\")\nplt.show()\n#we can see that graph goes nearly constant after kmeans = 5\n#so lets take number of clusters  = 5\nadd Codeadd Markdown\nkmeans = KMeans(n_clusters = 5)\n​\nlabel = kmeans.fit_predict(X3)\nprint(label)\nadd Codeadd Markdown\n#lets check the centroids now \nprint(kmeans.cluster_centers_)\nadd Codeadd Markdown\n#Now lets visualise the clusters we have formed\nclusters = kmeans.fit_predict(X3)\ndf[\"label\"] = clusters\n​\nfrom mpl_toolkits.mplot3d import Axes3D\n​\nfig = plt.figure(figsize = (20,10))\nax = fig.add_subplot(111,projection='3d')\nax.scatter(df.Age[df.label == 0], df[\"Annual Income (k$)\"][df.label == 0], df[\"Spending Score (1-100)\"][df.label == 0], c = 'blue' , s=60)\nax.scatter(df.Age[df.label == 1], df[\"Annual Income (k$)\"][df.label == 1], df[\"Spending Score (1-100)\"][df.label == 1], c = 'red' , s=60)\nax.scatter(df.Age[df.label == 2], df[\"Annual Income (k$)\"][df.label == 2], df[\"Spending Score (1-100)\"][df.label == 2], c = 'green' , s=60)\nax.scatter(df.Age[df.label == 3], df[\"Annual Income (k$)\"][df.label == 3], df[\"Spending Score (1-100)\"][df.label == 3], c = 'orange' , s=60)\nax.scatter(df.Age[df.label == 4], df[\"Annual Income (k$)\"][df.label == 4], df[\"Spending Score (1-100)\"][df.label == 4], c = 'purple' , s=60)\nax.view_init(30,185)\n​\nplt.xlabel(\"Age\")\nplt.ylabel(\"Annual Income (k$)\")\nax.set_zlabel('Spending score')\n​\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"2.Use Clustering Techniques for the any\ncustomer dataset using machine\nlearning","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n/kaggle/input/mall-customers/Mall_Customers.csv\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndata = pd.read_csv('../input/mall-customers/Mall_Customers.csv')\ndata.head()\ndata.shape\nx = data.iloc[:, 3:]\nx\nx1 = x.copy(deep=True)\nfrom sklearn.cluster import KMeans\nwcss = []\nfor i in range(1,15):\n    kmean = KMeans(n_clusters=i, init='k-means++', random_state=30)\n    kmean.fit(x)\n    wcss.append(kmean.inertia_)\nwcss\nplt.plot(range(1,15),wcss)\nkmean1 = KMeans(n_clusters=5, init='k-means++',random_state=30)\nkmean1.fit_predict(x)\nx['cluster number'] = kmean1.fit_predict(x)\nx\nx[x['cluster number'] == 4]\nx[x['cluster number'] == 3]\nx[x['cluster number'] == 0]\nx[x['cluster number'] == 2]   \nkmean1.predict([[55,31]])\nfrom sklearn.cluster import MiniBatchKMeans\nminibatch_kmean = MiniBatchKMeans(n_clusters=5)\nminibatch_kmean.fit(x1)\nminibatch_kmean.predict([[55, 31]])\n\nfrom sklearn.cluster import DBSCAN\ndbscan = DBSCAN(eps=1, min_samples=3)\ndbscan.fit(x1)\nset(dbscan.labels_)\nlen(set(dbscan.labels_))\nx1['cluster number'] = dbscan.labels_\nX1\nfrom sklearn import metrics\nmetrics.adjusted_rand_score(x['cluster number'], dbscan.labels_)\n\ntrue_label = x['cluster number']\npredicted_label = dbscan.labels_\nmetrics.jaccard_score(true_label, predicted_label, average='macro')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"1.Use classification technique for\nprediction of Graduate Admissions\nfrom an Indian perspective","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\ndf = pd.read_csv('/kaggle/input/graduate-admissions/Admission_Predict_Ver1.1.csv')\ndf\ny = df[\"Chance of Admit \"]\nx = df.drop([\"Chance of Admit \",\"Serial No.\"], axis=1)\nPrincipal Component Analysis\npca = PCA(n_components=2)\npca.fit(x)\nX_pca = pca.transform(x)\nX_pca_df = pd.DataFrame(X_pca, columns=[\"PC1\",\"PC2\"])\nX_pca_df\nplt.figure(figsize=(14,10))\nplt.scatter(X_pca_df[\"PC1\"], X_pca_df[\"PC2\"])\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nPCA_max = np.argmax(X_pca_df[\"PC1\"])\nPCA_min = np.argmin(X_pca_df[\"PC1\"])\nprint(\"PCA_Max :\", PCA_max)\nprint(\"PCA_min :\", PCA_min)\nx.iloc[PCA_max,:]\nx.iloc[PCA_min,:]\nScale Data\nscaler = MinMaxScaler()\nx = scaler.fit_transform(X_pca_df)\nX_train, X_test, y_train, y_test = train_test_split(x,y,train_size=0.8, shuffle=True,random_state=42)\nTrain Linear Regression Model\nlr = LinearRegression()\nlr.fit(X_train,y_train)\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n/kaggle/input/graduate-admissions/Admission_Predict.csv\n/kaggle/input/graduate-admissions/Admission_Predict_Ver1.1.csv\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nLoading Dataset\ndf = pd.read_csv('/kaggle/input/graduate-admissions/Admission_Predict_Ver1.1.csv')\ndf\nSerial No.\tGRE Score\tTOEFL Score\tUniversity Rating\tSOP\tLOR\tCGPA\tResearch\tChance of Admit\n0\t1\t337\t118\t4\t4.5\t4.5\t9.65\t1\t0.92\n1\t2\t324\t107\t4\t4.0\t4.5\t8.87\t1\t0.76\n2\t3\t316\t104\t3\t3.0\t3.5\t8.00\t1\t0.72\n3\t4\t322\t110\t3\t3.5\t2.5\t8.67\t1\t0.80\n4\t5\t314\t103\t2\t2.0\t3.0\t8.21\t0\t0.65\n...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n495\t496\t332\t108\t5\t4.5\t4.0\t9.02\t1\t0.87\n496\t497\t337\t117\t5\t5.0\t5.0\t9.87\t1\t0.96\n497\t498\t330\t120\t5\t4.5\t5.0\t9.56\t1\t0.93\n498\t499\t312\t103\t4\t4.0\t5.0\t8.43\t0\t0.73\n499\t500\t327\t113\t4\t4.5\t4.5\t9.04\t0\t0.84\n500 rows × 9 columns\n\ny = df[\"Chance of Admit \"]\nx = df.drop([\"Chance of Admit \",\"Serial No.\"], axis=1)\nPrincipal Component Analysis\npca = PCA(n_components=2)\npca.fit(x)\nX_pca = pca.transform(x)\nX_pca_df = pd.DataFrame(X_pca, columns=[\"PC1\",\"PC2\"])\nX_pca_df\nPC1\tPC2\n0\t-23.273887\t-0.783279\n1\t-6.811715\t3.351076\n2\t1.841991\t2.687708\n3\t-6.140811\t-0.024660\n4\t4.212021\t2.824404\n...\t...\t...\n495\t-14.484897\t5.912684\n496\t-22.957009\t-0.008506\n497\t-17.942750\t-5.731901\n498\t5.690921\t1.610405\n499\t-12.103936\t-0.724204\n500 rows × 2 columns\n\nplt.figure(figsize=(14,10))\nplt.scatter(X_pca_df[\"PC1\"], X_pca_df[\"PC2\"])\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nText(0, 0.5, 'PC2')\n\nPCA_max = np.argmax(X_pca_df[\"PC1\"])\nPCA_min = np.argmin(X_pca_df[\"PC1\"])\nprint(\"PCA_Max :\", PCA_max)\nprint(\"PCA_min :\", PCA_min)\nPCA_Max : 377\nPCA_min : 202\nx.iloc[PCA_max,:]\nGRE Score            290.00\nTOEFL Score          100.00\nUniversity Rating      1.00\nSOP                    1.50\nLOR                    2.00\nCGPA                   7.56\nResearch               0.00\nName: 377, dtype: float64\nx.iloc[PCA_min,:]\nGRE Score            340.00\nTOEFL Score          120.00\nUniversity Rating      5.00\nSOP                    4.50\nLOR                    4.50\nCGPA                   9.91\nResearch               1.00\nName: 202, dtype: float64\n#Scale Data\nscaler = MinMaxScaler()\nx = scaler.fit_transform(X_pca_df)\nX_train, X_test, y_train, y_test = train_test_split(x,y,train_size=0.8, shuffle=True,random_state=42)\nTrain Linear Regression Model\nlr = LinearRegression()\nlr.fit(X_train,y_train)\n\nLinearRegression\nLinearRegression()\n#LR Evaluation\nlr.score(X_test, y_test)\n0.660354611264309\npred = lr.predict(X_test)\nplt.figure(figsize=(9,6))\nplt.plot(pred,y_test, 'o')\nplt.xlabel('predicted Value')\nplt.ylabel('Actual Value')\nm, b = np.polyfit(pred, y_test, 1)\nplt.plot(pred, m*pred+b, color='red')\nplt.show()\n!pip install pycaret -q\n\nfrom pycaret.regression import *\nnew_df = X_pca_df.copy()\nnew_df[\"Chance\"] = y.tolist()\nsetup(data = new_df,target='Chance')    \ncompare_models()\n","metadata":{},"execution_count":null,"outputs":[]}]}